---
title: "678mid_project"
author: "xiaofei_wu"
date: "2019/11/19"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
  
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
system("java -version")

## Load libraries
## Here we load libraries for data wrangling and visualisation.

library(lme4)
library(jsonlite)
library(knitr)
library(forecast)
library(zoo)
library(magrittr)
library(tidyverse)
library(dplyr)
library(stringr)
<<<<<<< HEAD
=======
library(jsonlite)
>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8
install.packages('psych')
install.packages('GPArotation')
library(psych)
library(GPArotation)
```

## I. Load data
```{r}
# Random select 30000 rows from the training data set using ternimal
## use fread to produce data.table，then transfer it to data.frame.
fsample_select<-data.table::fread("data/sample_select.csv",select=c(1:6,8:13),stringsAsFactors=FALSE)
sample_select<-as.data.frame(fsample_select)
## renames the columns for the train and test dataset.
head<- read.csv("data/headfile.csv")
colnames(sample_select)<-colnames(head)[-7]
sample_select$fullVisitorId=as.character(sample_select$fullVisitorId)  #id as character
```



## II. Clean Data
## check mode of columns and split json columns 
```{r}
#split json columns 

data_full<- sample_select %>% 
  mutate(custom_dimensions_index=str_extract_all(customDimensions,"(?<=: \')(.*?)(?=\')")[[1]][1]) %>% 
  mutate(custom_dimensions_value=str_extract_all(customDimensions,"(?<=: \')(.*?)(?=\')")[[1]][2]) 
#replace quoates & split json columns
data1 <- data.frame(lapply(data_full, function(x){gsub("\"\"", "\"", x)}))
## split device columns. 
json_device <- paste("[",paste(data1$device,collapse = ","),"]")
device_add <- fromJSON(json_device)
## split geoNetwork columns. 
json_geo <- paste("[",paste(data1$geoNetwork,collapse = ","),"]")
geo_add <- fromJSON(json_geo)
## split totals columns. 
json_ttls <- paste("[",paste(unlist(data1$totals),collapse = ","),"]")
ttls_add <- fromJSON(json_ttls)
## split trafficSource columns. 
json_trSr <- paste("[",paste(data1$trafficSource,collapse = ","),"]")
trSr_add <- fromJSON(json_trSr,flatten = TRUE)
factor_full<-cbind(data1,device_add,geo_add,ttls_add,trSr_add)%>%select(-c(customDimensions,device,geoNetwork,totals,trafficSource))

## select the first 20000 rows as the training dataset 
## and select the last 10000 rows as the test dataset 
<<<<<<< HEAD
factor_full$pageviews<-as.numeric(factor_full$pageviews)
factor_full$visitNumber<-as.numeric(factor_full$visitNumber)
factor_full$transactionRevenue<-as.numeric(factor_full$transactionRevenue)
factor_train_use$transactionRevenue<-as.numeric(factor_train_use$transactionRevenue)
factor_full$transactionRevenue[is.na(factor_full$transactionRevenue)] <- 0
factor_full$pageviews[is.na(factor_full$pageviews)] <- 0
factor_full$newVisits[is.na(factor_full$newVisits)] <- 0
factor_full$isTrueDirect[is.na(factor_full$isTrueDirect)] <- FALSE
factor_full<-factor_full%>%mutate(wkday=as.character(lubridate::wday(ymd(date)-1)))
=======
>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8
factor_train<- factor_full[1:20000,]
factor_test<- factor_full[20001:30000,]
write_csv(factor_train,"/Users/vv/Desktop/678/mid_project/midterm_project_678/data/datatrain_factors")
write_csv(factor_test,"/Users/vv/Desktop/678/mid_project/midterm_project_678/data/datatest_factors")
<<<<<<< HEAD
# none zero revenue "factor_train" to be factor_train_use
factor_train_use <- factor_train[factor_train$transactionRevenue!=0,]
factor_test_use <- factor_test[factor_test$transactionRevenue!=0,]
=======
>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8
```

## III. Exploratory Data Analysis
## analyze each columns 
```{r}
## Data Fields
# fullVisitorId- A unique identifier for each user of the Google Merchandise Store.
# channelGrouping - The channel via which the user came to the Store.
# date - The date on which the user visited the Store.
# device - The specifications for the device used to access the Store.
# geoNetwork - This section contains information about the geography of the user.
# socialEngagementType - Engagement type, either "Socially Engaged" or "Not Socially Engaged".
# totals - This section contains aggregate values across the session.
# trafficSource - This section contains information about the Traffic Source from which the session originated.
# visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.
# visitNumber - The session number for this user. If this is the first session, then this is set to 1.
# visitStartTime - The timestamp (expressed as POSIX time).
# hits - This row and nested fields are populated for any and all types of hits. Provides a record of all page visits.
# customDimensions - This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set.
# totals - This set of columns mostly includes high-level aggregate data.
```

```{r}
## Distribution of the revenue
hist(x=as.numeric(factor_train$transactionRevenue),,main="Distribution of the revenue", xlab = "revenue")
## We notive most of the transactionRevenue laid arount zero, so we want to check if they are actually zero. 
## The percentage of the non-zero and zero Revenue.
contain_revenue=factor_train %>% 
  mutate(Revenue=ifelse(transactionRevenue>0,1,0)) %>% 
  group_by(Revenue) %>% 
  summarise(n=n()) %>% 
  mutate(percentage=paste0(round(n/nrow(factor_train),4)*100,"%"))
contain_revenue

## We can see that only 1.1% percentage of the data was none-zero revenue
## Also most of the rows are zero revenue, we will focus on the none-zero revenue history to do the predict. 
factor_train_fo<-factor_train%>%filter(!is.na(factor_train$transactionRevenue))
hist(x=as.numeric(factor_train_fo$transactionRevenue),main="Distribution of the revenue for none-zero", xlab = "revenue for none-zero")

## For our none-zero revenue data, the distribution of revenue is right skewed, so we see the log(revenue distribution). 
hist(x=log(as.numeric(factor_train_fo$transactionRevenue)),main="Distribution of the log revenue for none-zero", xlab = "log revenue for none-zero")
## For nonzero targets, they seems follow normal distribution.
## Thus I created qqplot, and it indicates that the nonzero target approximately follows normal distribution.
y=log(as.numeric(factor_train_fo$transactionRevenue))
qqnorm(y);qqline(y, col = 2,lwd=2,lty=2)
```
## deleate columns that do not provide valuable content
```{r}
col_name=colnames(factor_train_fo)
# uniques<-list()
# for(i in 1:59){
#   column=colnames(factor_train_fo)[i]
#   print(column)
#  print(unique(factor_train_fo$column))
# }
unique(factor_train_fo$custom_dimensions_index)#ALL IS 4, SO DELEATE COLUMN
unique(factor_train_fo$socialEngagementType) #ALL IS "Not Socially Engaged", SO DELEATE COLUMN
unique(factor_train_fo$custom_dimensions_value) #ALL IS "North America", SO DELEATE COLUMN
unique(factor_train_fo$visits) #ALL IS 1, SO DELEATE COLUMN
##DELEATE following columns since they do not provide valuable content. "browserVersion", "browserSize", "operatingSystemVersion", "mobileDeviceBranding", "mobileDeviceModel", "mobileInputSelector", "mobileDeviceInfo", "mobileDeviceMarketingName", "flashVersion", "language", "screenColors", "screenResolution", "cityId", "latitude", "longitude", "networkLocation", "bounces"
factor_train_use<-select(factor_train_fo,c(1,2,3,5:7,10,13,15,25:31, 33,38:41,43:46))
factor_train<-select(factor_train_use,c(-c(2:4),-6,-21,-22))
## transfer all valuables into numerical,do the factor analysis to see the major group of predictors. 
factor_train_num<-factor_train%>%transform(channelGrouping=as.numeric(channelGrouping),visitNumber=as.numeric(visitNumber), browser =as.numeric(as.factor(browser)),operatingSystem =as.numeric(as.factor(operatingSystem)),isMobile =as.numeric(isMobile),deviceCategory =as.numeric(as.factor(deviceCategory)),continent =as.numeric(as.factor(continent)),subContinent =as.numeric(as.factor(subContinent)),country =as.numeric(as.factor(country)),region =as.numeric(as.factor(region)),metro =as.numeric(as.factor(metro)),city =as.numeric(as.factor(city)),networkDomain =as.numeric(as.factor(networkDomain)),hits =as.numeric(as.factor(hits)),pageviews =as.numeric(as.factor(pageviews)),timeOnSite =as.numeric(as.factor(timeOnSite)),transactions =as.numeric(as.factor(transactions)),transactionRevenue =as.numeric(as.factor(transactionRevenue)),totalTransactionRevenue =as.numeric(as.factor(totalTransactionRevenue)))
factor_train_num<-as.data.frame(factor_train_num)
## correlation between predictors
factor_train_cor<-cor(factor_train_num, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))
Percentile_00  = min(abs(factor_train_cor))
Percentile_33  = quantile(abs(factor_train_cor), 0.33333)
Percentile_67  = quantile(abs(factor_train_cor), 0.66667)
Percentile_100 = quantile(abs(factor_train_cor), 0.99999)
<<<<<<< HEAD
##  transfer the lower 66.7% correlation to NI (Not importment)
factor_train_cor[abs(factor_train_cor) >= Percentile_00 & factor_train_cor <= Percentile_67] = "NI"
factor_train_cor
=======

## Only transfer the top 1/3 correlation to upper
factor_train_cor[abs(factor_train_cor) >= Percentile_67 & factor_train_cor <= Percentile_100] = "Upper"

>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8

## try factor analysis between predictors
parallel <- fa.parallel(factor_train_num, fm = 'minres', fa = 'fa')
## Parallel analysis suggests that the number of factors =  6  and the number of components =  NA 
sixfactor <- fa(factor_train_num,nfactors = 6,rotate = "oblimin",fm="minres")
<<<<<<< HEAD
# print(sixfactor)
=======
print(sixfactor)
>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8
fa.diagram(sixfactor)
# Despide our output revenue, we could see that the factor analysis suggests to put all of our predictors into 5 groups, I conclude as "device", "hits", "city", "continent", "region+browser"
# the conbination of region and browser is wierd, so I also tried the number of factors =  5
fivefactor <- fa(factor_train_num,nfactors = 5,rotate = "oblimin",fm="minres")
<<<<<<< HEAD
# print(fivefactor)
=======
print(fivefactor)
>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8
fa.diagram(fivefactor)
# for the number of factors =  5, we could see that the factor analysis suggests to put all of our predictors into"device+browser", "hits", "city", "continent", which make more sense here. 

unique(factor_train$isMobile)
unique(factor_train$deviceCategory)

<<<<<<< HEAD

# factor_train$transactionRevenue<-as.numeric(factor_train$transactionRevenue)
# factor_train_use$transactionRevenue<-as.numeric(factor_train_use$transactionRevenue)
# factor_train$transactionRevenue[is.na(factor_train$transactionRevenue)] <- 0
# factor_train$pageviews[is.na(factor_train$pageviews)] <- 0
# factor_train$newVisits[is.na(factor_train$newVisits)] <- 0
# factor_train$isTrueDirect[is.na(factor_train$isTrueDirect)] <- FALSE
#pageviews
ggplot(factor_train, aes(x=pageviews, y=log(as.numeric(transactionRevenue)+1), group=fullVisitorId,color=fullVisitorId))+
  geom_smooth(method="lm")+
  ylab("Revenue")+
  ggtitle("Revenue v.s. pageviews per user")+
  guides(color=FALSE)+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.title.x = element_text(face="bold",  size=15), axis.title.y = element_text(face="bold",  size=15),plot.title = element_text(size=15, face="bold"),  axis.text.x  = element_text(angle=45,vjust=0.5, size=10))
## CONCLUSION: the slope of “page views” will change a lot between different users
## adding (0+pageviews|fullVisitorId) in the linear mixed models.

# newVisits: “new visits” has great impact on outcome, thus it should be included in the model.
T_nVisits <- factor_train %>% 
  select(newVisits,transactionRevenue) %>% 
  group_by(newVisits) %>% 
  summarise(mean_revenue=mean(log(transactionRevenue+1))) %>% 
  arrange(desc(mean_revenue))
ggplot(T_nVisits,mapping = aes(x=reorder(newVisits,desc(mean_revenue)), y=mean_revenue))+
  geom_bar(fill="skyblue",stat="identity")+
  geom_text(aes(label=round(mean_revenue,2)), vjust=0)+
  xlab("new visits")+
  theme(axis.text.x = element_text(angle=45))+
  ggtitle("mean revenue v.s. new visits")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.title.x = element_text(face="bold",  size=15), axis.title.y = element_text(face="bold",  size=15),plot.title = element_text(size=15, face="bold"),  axis.text.x  = element_text(angle=45,vjust=0.5, size=10))

# visitNumber
T_nVisnum <- factor_train_use %>% 
  select(visitNumber,transactionRevenue) %>% 
  group_by(visitNumber) %>% 
  summarise(mean_revenue=mean(log(transactionRevenue+1))) %>% 
  arrange(desc(mean_revenue))
ggplot(T_nVisnum,mapping = aes(x=visitNumber, y=mean_revenue))+
    geom_smooth()+
  geom_point()+geom_jitter()+
  theme(axis.text.x = element_text(angle=45))+
  ggtitle("mean revenue v.s. visitNumber")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.title.x = element_text(face="bold",  size=15), axis.title.y = element_text(face="bold",  size=15),plot.title = element_text(size=15, face="bold"),  axis.text.x  = element_text(angle=45,vjust=0.5, size=10))

#weekday
# 1 means Monday, 7 means Sunday (default).
install.packages("lubridate")
library("lubridate")
factor_train_wday <- factor_train %>% 
  select(date,transactionRevenue) %>% 
  mutate(wkday=as.character(lubridate::wday(ymd(date)-1)))
T_wkday <- factor_train_wday %>% 
  group_by(wkday) %>% 
  summarise(mean_revenue=mean(log(transactionRevenue+1))) %>%
  arrange(desc(mean_revenue))
ggplot(T_wkday,mapping = aes(x=wkday, y=mean_revenue))+
  geom_bar(fill="skyblue",stat="identity")+
  geom_text(aes(label=round(mean_revenue,2)), vjust=0)+
  theme(axis.text.x = element_text(angle=45))+
  ggtitle("mean revenue v.s. weekday")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.title.x = element_text(face="bold",  size=15), axis.title.y = element_text(face="bold",  size=15),plot.title = element_text(size=15, face="bold"),  axis.text.x  = element_text(angle=45,vjust=0.5, size=10))
factor_train<-factor_train%>%mutate(wkday=as.character(lubridate::wday(ymd(date)-1)))
factor_train_use<-factor_train_use%>%mutate(wkday=as.character(lubridate::wday(ymd(date)-1)))
factor_test<-factor_test%>%mutate(wkday=as.character(lubridate::wday(ymd(date)-1)))
## recode by mean revenue

```
## II. Model
## 1. Mixed effect Linear Model (with log transformation)
```{r}
# by users
factor_train$pageviews<-as.numeric(factor_train$pageviews)
factor_train$visitNumber<-as.numeric(factor_train$visitNumber)
MD1_nolog <- lmer(transactionRevenue~(1|fullVisitorId)+(1|pageviews)+factor(browser)+scale(pageviews)+factor(newVisits)+scale(visitNumber)+factor(operatingSystem)+factor(isMobile)+factor(continent)+factor(isTrueDirect)+factor(wkday), data=factor_train)
summary(MD1_nolog)
plot(MD1_nolog,col="blue")
## residuals spread out, using LOG TRANSFORMATION.
MD1 <- lmer(log(transactionRevenue+1)~(1|fullVisitorId)+(1|pageviews)+factor(browser)+scale(pageviews)+factor(newVisits)+scale(visitNumber)+factor(operatingSystem)+factor(isMobile)+factor(continent)+factor(isTrueDirect)+factor(wkday), data=factor_train_use)
summary(MD1)
MD1.2 <- lmer(log(transactionRevenue+1)~(1|fullVisitorId)+(1|pageviews)+factor(browser)+scale(pageviews)+factor(newVisits)+scale(visitNumber)+factor(operatingSystem)+factor(isMobile)+factor(continent)+factor(isTrueDirect)+factor(wkday), data=factor_train)
summary(MD1.2)
##  the residual for none-zero revenue is 0.9766, which is pretty small
##  the residual for all revenue is 2.1965

#check model
plot(MD1,col="blue")
qqnorm(resid(MD1))
qqline(resid(MD1))
plot(MD1.2,col="blue")
qqnorm(resid(MD1.2))
qqline(resid(MD1.2))

#prediction using TEST
#weekday
# 1 means Monday, 7 means Sunday (default).
install.packages("lubridate")
library("lubridate")
factor_test$pageviews<-as.numeric(factor_test$pageviews)
factor_test$visitNumber<-as.numeric(factor_test$visitNumber)
factor_test_wday <- factor_test %>% 
  select(date,transactionRevenue) %>% 
  mutate(wkday=as.character(lubridate::wday(ymd(date)-1)))
T_wkday <- factor_test_wday %>% 
  group_by(wkday) %>% 
  summarise(mean_revenue=mean(log(transactionRevenue+1))) %>%
  arrange(desc(mean_revenue))
T_predictors1 <- T_predictors %>%
  mutate(revenue=log(transactionRevenue+1))
test_f <- factor_test %>%
  select(transactionRevenue,fullVisitorId,pageviews, bounces, visitNumber,newVisits, isMobile, isTrueDirect, browser, operatingSystem, wkday, continent,timeOnSite) %>%
  mutate(revenue=log(transactionRevenue+1))
test_f$transactionRevenue <- log(test_f$transactionRevenue+1)
test_f$pageviews <- scale(test_f$pageviews)
test_f$bounces <- scale(test_f$bounces)
test_f$visitNumber <- scale(test_f$visitNumber)
test_f$isTrueDirect <- as.factor(test_f$isTrueDirect)
test_f$timeOnSite <- scale(test_f$timeOnSite)
#train
pre_MD1.2=predict(MD1.2)
rmse_MD1.2=sqrt(mean((abs(pre_MD1.2-factor_train$transactionRevenue))^2))
rmse_MD1.2
#test
pre_MD1=predict(MD1,factor_test_use,allow.new.levels=TRUE)
rmse_MD1=sqrt(mean((abs(exp(pre_MD1)-factor_test_use$transactionRevenue))^2))

## bu continent 
MD2 <- lmer(log(transactionRevenue+1)~(1|continent)+(0+pageviews|continent)+factor(bounces)+factor(browser)+scale(pageviews)+factor(newVisits)+scale(visitNumber)+factor(operatingSystem)+factor(isMobile)+factor(isTrueDirect)+factor(wkday), data=factor_train_use)
summary(MD2)
MD2.2 <- lmer(log(transactionRevenue+1)~(1|continent)+(0+pageviews|continent)+factor(browser)+scale(pageviews)+factor(newVisits)+scale(visitNumber)+factor(operatingSystem)+factor(isMobile)+factor(continent)+factor(isTrueDirect)+factor(wkday), data=factor_train)
summary(MD2.2)
#check model
plot(MD2,col="blue")
qqnorm(resid(MD2))
qqline(resid(MD2))
plot(MD2.2,col="blue")
qqnorm(resid(MD2.2))
qqline(resid(MD2.2))

#train
pre_MD2.2=predict(MD2.2)
rmse_MD2.2=sqrt(mean((abs(pre_MD2.2-factor_train$transactionRevenue))^2))
rmse_MD2.2
#test
pre_MD1=predict(MD1,factor_test_use,allow.new.levels=TRUE)
rmse_MD1=sqrt(mean((abs(exp(pre_MD1)-factor_test_use$transactionRevenue))^2))
```

## 2. Logistic Model 
# use to predict if the transectionRevenue == 0
```{r}
#data --logis
library(dplyr)
library(tydverse)
train_logis <- factor_train %>%
  mutate(iftransaction=ifelse(transactionRevenue==0,0,1))
test_logis <- factor_test %>%
  mutate(iftransaction=ifelse(transactionRevenue==0,0,1))


## MD3: EDA with interaction
MD3 <- glm(iftransaction~scale(pageviews)+factor(newVisits)+factor(browser)+factor(operatingSystem)+factor(isMobile)+factor(continent)+factor(wkday)+factor(isTrueDirect)+scale(visitNumber)+pageviews*browser+visitNumber*operatingSystem+visitNumber*isMobile+visitNumber*isTrueDirect+pageviews*operatingSystem+visitNumber*browser,family=binomial(link="logit"),data=train_logis)
summary(MD3)

#### train
pre_logis_MD3_train <- round(predict(MD3,type="response"),0)
sum(pre_logis_MD3_train==train_logis$iftransaction)/nrow(train_logis)
#   0.98895
test_logis<-test_logis[-c(test_logis$browser=="Nintendo Browser"),]
test_logis<-test_logis[!test_logis$browser=="Nintendo Browser",]
pre_logis_MD2_test <- round(predict(MD3,test_logis,type="response"),0)
sum(pre_logis_MD2_test==test_logis$iftransaction)/nrow(test_logis)
##  0.9887933
sum(pre_logis_MD3_train==1 & train_logis$iftransaction==1)
sum(train_logis$iftransaction==1)

sum(pre_logis_MD2_test==1 & test_logis$iftransaction==1)

factor_test_use1<-factor_test_use[pre_logis_MD2_test!=0,]
factor_train_use <- factor_train[factor_train$transactionRevenue!=0,]
logis_MD2_test=predict(MD1,factor_test_use1,allow.new.levels=TRUE)
rmse_MD1=sqrt(mean((abs(pre_MD1-factor_test_use$transactionRevenue))^2))
```
=======
```
>>>>>>> 6d2eaa3b87c401c1b510f3c104d7e237570fa0f8
